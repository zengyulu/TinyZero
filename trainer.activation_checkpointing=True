2025-02-01 10:12:52,482	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(main_task pid=48777)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=48777)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=48777)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=48777)[0m                                                  'grad_offload': False,
[36m(main_task pid=48777)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=48777)[0m                                                  'param_offload': False,
[36m(main_task pid=48777)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=48777)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=48777)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=48777)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=48777)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=48777)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=48777)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=48777)[0m                                            'total_training_steps': -1,
[36m(main_task pid=48777)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=48777)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=48777)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=48777)[0m                                  'ppo_micro_batch_size': 4,
[36m(main_task pid=48777)[0m                                  'ppo_mini_batch_size': 64,
[36m(main_task pid=48777)[0m                                  'shuffle': False,
[36m(main_task pid=48777)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=48777)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=48777)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=48777)[0m                                  'use_kl_loss': False},
[36m(main_task pid=48777)[0m                        'hybrid_engine': True,
[36m(main_task pid=48777)[0m                        'model': {'enable_gradient_checkpointing': False,
[36m(main_task pid=48777)[0m                                  'external_lib': None,
[36m(main_task pid=48777)[0m                                  'override_config': {},
[36m(main_task pid=48777)[0m                                  'path': '/home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B',
[36m(main_task pid=48777)[0m                                  'use_remove_padding': False},
[36m(main_task pid=48777)[0m                        'ref': {'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=48777)[0m                                                'param_offload': False,
[36m(main_task pid=48777)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=48777)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=48777)[0m                                'log_prob_micro_batch_size': 4,
[36m(main_task pid=48777)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=48777)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=48777)[0m                        'rollout': {'do_sample': True,
[36m(main_task pid=48777)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=48777)[0m                                    'enforce_eager': True,
[36m(main_task pid=48777)[0m                                    'free_cache_engine': True,
[36m(main_task pid=48777)[0m                                    'gpu_memory_utilization': 0.35,
[36m(main_task pid=48777)[0m                                    'ignore_eos': False,
[36m(main_task pid=48777)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=48777)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=48777)[0m                                    'log_prob_micro_batch_size': 4,
[36m(main_task pid=48777)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=48777)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=48777)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=48777)[0m                                    'n': 1,
[36m(main_task pid=48777)[0m                                    'name': 'vllm',
[36m(main_task pid=48777)[0m                                    'prompt_length': 256,
[36m(main_task pid=48777)[0m                                    'response_length': 1024,
[36m(main_task pid=48777)[0m                                    'temperature': 1.0,
[36m(main_task pid=48777)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=48777)[0m                                    'top_k': -1,
[36m(main_task pid=48777)[0m                                    'top_p': 1}},
[36m(main_task pid=48777)[0m  'algorithm': {'adv_estimator': 'gae',
[36m(main_task pid=48777)[0m                'gamma': 1.0,
[36m(main_task pid=48777)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=48777)[0m                'kl_penalty': 'kl',
[36m(main_task pid=48777)[0m                'lam': 1.0},
[36m(main_task pid=48777)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=48777)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=48777)[0m             'forward_micro_batch_size': 8,
[36m(main_task pid=48777)[0m             'grad_clip': 1.0,
[36m(main_task pid=48777)[0m             'model': {'enable_gradient_checkpointing': False,
[36m(main_task pid=48777)[0m                       'external_lib': None,
[36m(main_task pid=48777)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=48777)[0m                                       'grad_offload': False,
[36m(main_task pid=48777)[0m                                       'optimizer_offload': False,
[36m(main_task pid=48777)[0m                                       'param_offload': False,
[36m(main_task pid=48777)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=48777)[0m                       'override_config': {},
[36m(main_task pid=48777)[0m                       'path': '/home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B',
[36m(main_task pid=48777)[0m                       'tokenizer_path': '/home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B',
[36m(main_task pid=48777)[0m                       'use_remove_padding': False},
[36m(main_task pid=48777)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=48777)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=48777)[0m                       'min_lr_ratio': None,
[36m(main_task pid=48777)[0m                       'total_training_steps': -1,
[36m(main_task pid=48777)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=48777)[0m             'ppo_epochs': 1,
[36m(main_task pid=48777)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=48777)[0m             'ppo_micro_batch_size': 8,
[36m(main_task pid=48777)[0m             'ppo_mini_batch_size': 64,
[36m(main_task pid=48777)[0m             'shuffle': False,
[36m(main_task pid=48777)[0m             'strategy': 'fsdp',
[36m(main_task pid=48777)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=48777)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=48777)[0m  'data': {'max_prompt_length': 256,
[36m(main_task pid=48777)[0m           'max_response_length': 1024,
[36m(main_task pid=48777)[0m           'prompt_key': 'prompt',
[36m(main_task pid=48777)[0m           'return_raw_chat': False,[36m(main_task pid=48777)[0m /home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=48777)[0m No module named 'vllm._version'
[36m(main_task pid=48777)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=49176)[0m /home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=49176)[0m No module named 'vllm._version'[32m [repeated 2x across cluster][0m
[36m(pid=49176)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=49176)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=49176)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=49176)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at /home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B and are newly initialized: ['score.bias']
[36m(WorkerDict pid=49176)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=48973)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at /home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B and are newly initialized: ['score.bias', 'score.weight']

[36m(main_task pid=48777)[0m           'return_raw_input_ids': False,
[36m(main_task pid=48777)[0m           'tokenizer': None,
[36m(main_task pid=48777)[0m           'train_batch_size': 32,
[36m(main_task pid=48777)[0m           'train_files': '/home/zengyu/Workspace/0_git/TinyZero/data/train/train.parquet',
[36m(main_task pid=48777)[0m           'val_batch_size': 328,
[36m(main_task pid=48777)[0m           'val_files': '/home/zengyu/Workspace/0_git/TinyZero/data/train/test.parquet'},
[36m(main_task pid=48777)[0m  'reward_model': {'enable': False,
[36m(main_task pid=48777)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=48777)[0m                   'max_length': None,
[36m(main_task pid=48777)[0m                   'micro_batch_size': 64,
[36m(main_task pid=48777)[0m                   'model': {'external_lib': None,
[36m(main_task pid=48777)[0m                             'fsdp_config': {'min_num_params': 0,
[36m(main_task pid=48777)[0m                                             'param_offload': False},
[36m(main_task pid=48777)[0m                             'input_tokenizer': '/home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B',
[36m(main_task pid=48777)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=48777)[0m                             'use_remove_padding': False},
[36m(main_task pid=48777)[0m                   'strategy': 'fsdp',
[36m(main_task pid=48777)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=48777)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=48777)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=48777)[0m              'default_hdfs_dir': None,
[36m(main_task pid=48777)[0m              'default_local_dir': 'checkpoints/TinyZero/countdown-qwen2.5-0.5b',
[36m(main_task pid=48777)[0m              'experiment_name': 'countdown-qwen2.5-0.5b',
[36m(main_task pid=48777)[0m              'logger': ['wandb'],
[36m(main_task pid=48777)[0m              'n_gpus_per_node': 2,
[36m(main_task pid=48777)[0m              'nnodes': 1,
[36m(main_task pid=48777)[0m              'project_name': 'TinyZero',
[36m(main_task pid=48777)[0m              'save_freq': 100,
[36m(main_task pid=48777)[0m              'test_freq': 100,
[36m(main_task pid=48777)[0m              'total_epochs': 15,
[36m(main_task pid=48777)[0m              'total_training_steps': None,
[36m(main_task pid=48777)[0m              'val_before_train': False}}
[36m(main_task pid=48777)[0m original dataset len: 327680
[36m(main_task pid=48777)[0m filter dataset len: 327680
[36m(main_task pid=48777)[0m original dataset len: 1024
[36m(main_task pid=48777)[0m filter dataset len: 1024
[36m(main_task pid=48777)[0m Size of train dataloader: 10240
[36m(main_task pid=48777)[0m Size of val dataloader: 1
[36m(main_task pid=48777)[0m Total training steps: 153600
[36m(WorkerDict pid=48973)[0m Critic overriding config {'bos_token_id': None, 'eos_token_id': 151643, 'pad_token_id': 151643}
[36m(WorkerDict pid=48973)[0m Qwen2ForTokenClassification contains 1.54B parameters
[36m(WorkerDict pid=48973)[0m Before critic FSDP, memory allocated (GB): 0.0, memory reserved (GB): 0.0
[36m(WorkerDict pid=48973)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=48973)[0m After critic FSDP, memory allocated (GB): 2.896498203277588, memory reserved (GB): 6.060546875
[36m(WorkerDict pid=48973)[0m Total steps: 153600, num_warmup_steps: 0
[36m(WorkerDict pid=48973)[0m Critic use_remove_padding=False
[36m(WorkerDict pid=48973)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=48973)[0m   "_name_or_path": "/home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B",
[36m(WorkerDict pid=48973)[0m   "architectures": [
[36m(WorkerDict pid=48973)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=48973)[0m   ],
[36m(WorkerDict pid=48973)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=48973)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=48973)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=48973)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=48973)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=48973)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=48973)[0m   "max_position_embeddings": 131072,
[36m(WorkerDict pid=48973)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=48973)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=48973)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=48973)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=48973)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=48973)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=48973)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=48973)[0m   "rope_scaling": null,
[36m(WorkerDict pid=48973)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=48973)[0m   "sliding_window": null,
[36m(WorkerDict pid=48973)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=48973)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=48973)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=48973)[0m   "use_cache": true,
[36m(WorkerDict pid=48973)[0m   "use_mrope": false,
[36m(WorkerDict pid=48973)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=48973)[0m   "vocab_size": 151936
[36m(WorkerDict pid=48973)[0m }
[36m(WorkerDict pid=48973)[0m 
[36m(WorkerDict pid=48973)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=48973)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x78e235b3b240>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})
[36m(WorkerDict pid=48973)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=48973)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=48973)[0m   "_name_or_path": "/home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B",
[36m(WorkerDict pid=48973)[0m   "architectures": [
[36m(WorkerDict pid=48973)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=48973)[0m   ],
[36m(WorkerDict pid=48973)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=48973)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=48973)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=48973)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=48973)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=48973)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=48973)[0m   "max_position_embeddings": 131072,
[36m(WorkerDict pid=48973)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=48973)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=48973)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=48973)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=48973)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=48973)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=48973)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=48973)[0m   "rope_scaling": null,
[36m(WorkerDict pid=48973)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=48973)[0m   "sliding_window": null,
[36m(WorkerDict pid=48973)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=48973)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=48973)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=48973)[0m   "use_cache": true,
[36m(WorkerDict pid=48973)[0m   "use_mrope": false,
[36m(WorkerDict pid=48973)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=48973)[0m   "vocab_size": 151936
[36m(WorkerDict pid=48973)[0m }
[36m(WorkerDict pid=48973)[0m 
[36m(WorkerDict pid=48973)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=48973)[0m Before building vllm rollout, memory allocated (GB): 7.230706691741943, memory reserved (GB): 10.43359375
[36m(WorkerDict pid=48973)[0m INFO 02-01 10:13:08 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=48973)[0m WARNING 02-01 10:13:08 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=48973)[0m local rank 0
[36m(WorkerDict pid=48973)[0m INFO 02-01 10:13:08 selector.py:115] Using XFormers backend.[36m(WorkerDict pid=48973)[0m /home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=48973)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=48973)[0m /home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=48973)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=49176)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=48973)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=48973)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=48973)[0m /home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=48973)[0m   warnings.warn(
[36m(main_task pid=48777)[0m wandb: Currently logged in as: zengyu_lu (finmark) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=48777)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(main_task pid=48777)[0m wandb: Tracking run with wandb version 0.19.5
[36m(main_task pid=48777)[0m wandb: Run data is saved locally in /home/zengyu/Workspace/0_git/TinyZero/wandb/run-20250201_101311-hktntlpf
[36m(main_task pid=48777)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=48777)[0m wandb: Syncing run countdown-qwen2.5-0.5b
[36m(main_task pid=48777)[0m wandb: ⭐️ View project at https://wandb.ai/finmark/TinyZero
[36m(main_task pid=48777)[0m wandb: 🚀 View run at https://wandb.ai/finmark/TinyZero/runs/hktntlpf

[36m(WorkerDict pid=49176)[0m Total steps: 153600, num_warmup_steps: 0[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=49176)[0m Critic use_remove_padding=False
[36m(WorkerDict pid=49176)[0m wrap_policy: functools.partial(<function transformer_auto_wrap_policy at 0x7b3338943240>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=48973)[0m INFO 02-01 10:13:08 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=48973)[0m INFO 02-01 10:13:08 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=48973)[0m INFO 02-01 10:13:08 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x78e1f13a9e50>, local_subscribe_port=34825, remote_subscribe_port=None)
[36m(WorkerDict pid=49176)[0m Actor use_remove_padding=False[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=48973)[0m before init cache memory allocated: 9.353723904GB, reserved: 9.498001408GB
[36m(WorkerDict pid=48973)[0m after init cache memory allocated: 14.169243648GB, reserved: 14.371782656GB
[36m(WorkerDict pid=48973)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 1024, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=48973)[0m After building vllm rollout, memory allocated (GB): 11.7546968460083, memory reserved (GB): 13.384765625
[36m(WorkerDict pid=48973)[0m After building sharding manager, memory allocated (GB): 11.7546968460083, memory reserved (GB): 13.384765625
[36m(main_task pid=48777)[0m epoch 0, step 1
[36m(main_task pid=48777)[0m --------------------------------
[36m(main_task pid=48777)[0m Target: 23 | Numbers: [63  3  7 17]
[36m(main_task pid=48777)[0m Extracted equation: None
[36m(main_task pid=48777)[0m Solution string: A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.
[36m(main_task pid=48777)[0m User: Using the numbers [63, 3, 7, 17], create an equation that equals 23. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 </answer>.
[36m(main_task pid=48777)[0m Assistant: Let me solve this step by step.
[36m(main_task pid=48777)[0m <think>63, 3, 7, 17
[36m(main_task pid=48777)[0m 23 - 17 = 6
[36m(main_task pid=48777)[0m 6 - 3 = 3
[36m(main_task pid=48777)[0m     1 * 3 = 3
[36m(main_task pid=48777)[0m  The equation that equals 23 is: (2 - 1) * 3 + 7 - 17 = 23
[36m(main_task pid=48777)[0m </answer><|endoftext|>
[36m(main_task pid=48777)[0m No equation found
[36m(WorkerDict pid=49176)[0m INFO 02-01 10:13:08 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=49176)[0m WARNING 02-01 10:13:08 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=49176)[0m local rank 0
[36m(WorkerDict pid=49176)[0m INFO 02-01 10:13:08 selector.py:115] Using XFormers backend.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=49176)[0m INFO 02-01 10:13:08 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=49176)[0m INFO 02-01 10:13:08 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=49176)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 1024, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
Error executing job with overrides: ['data.train_files=/home/zengyu/Workspace/0_git/TinyZero/data/train/train.parquet', 'data.val_files=/home/zengyu/Workspace/0_git/TinyZero/data/train/test.parquet', 'data.train_batch_size=32', 'data.val_batch_size=328', 'data.max_prompt_length=256', 'data.max_response_length=1024', 'actor_rollout_ref.model.path=/home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.actor.ppo_mini_batch_size=64', 'actor_rollout_ref.actor.ppo_micro_batch_size=4', 'actor_rollout_ref.rollout.log_prob_micro_batch_size=4', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.35', 'actor_rollout_ref.ref.log_prob_micro_batch_size=4', 'critic.optim.lr=1e-5', 'critic.model.path=/home/zengyu/Workspace/2_models/Qwen2/Qwen2.5-1.5B', 'critic.ppo_micro_batch_size=8', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.logger=[wandb]', '+trainer.val_before_train=False', 'trainer.default_hdfs_dir=null', 'trainer.n_gpus_per_node=2', 'trainer.nnodes=1', 'trainer.save_freq=100', 'trainer.test_freq=100', 'trainer.project_name=TinyZero', 'trainer.experiment_name=countdown-qwen2.5-0.5b', 'trainer.total_epochs=15']
Traceback (most recent call last):
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/trainer/main_ppo.py", line 103, in main
    ray.get(main_task.remote(config))
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::main_task()[39m (pid=48777, ip=192.168.178.104)
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/trainer/main_ppo.py", line 189, in main_task
    trainer.fit()
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/trainer/ppo/ray_trainer.py", line 649, in fit
    critic_output = self.critic_wg.update_critic(batch)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.critic_update_critic()[39m (pid=48973, ip=192.168.178.104, actor_id=63ee501757547f0edf2d4a7e01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x78e208f8f390>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/workers/fsdp_workers.py", line 713, in update_critic
    metrics = self.critic.update_critic(data=data)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/workers/critic/dp_critic.py", line 180, in update_critic
    vpreds = self._forward_micro_batch(data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/Workspace/0_git/TinyZero/verl/workers/critic/dp_critic.py", line 95, in _forward_micro_batch
    output = self.critic_module(input_ids=input_ids,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1380, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 895, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 620, in forward
    hidden_states = self.input_layernorm(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 79, in forward
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
               ^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 29.62 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 23.03 GiB is allocated by PyTorch, and 149.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(main_task pid=48777)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.critic_update_critic()[39m (pid=49176, ip=192.168.178.104, actor_id=a64b9bfa8de2edf8d5cf482a01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7b330894bd90>)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/Workspace/0_git/TinyZero/verl/single_controller/ray/base.py", line 399, in func
[36m(main_task pid=48777)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/Workspace/0_git/TinyZero/verl/single_controller/base/decorator.py", line 404, in inner
[36m(main_task pid=48777)[0m     return func(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/Workspace/0_git/TinyZero/verl/workers/fsdp_workers.py", line 713, in update_critic
[36m(main_task pid=48777)[0m     metrics = self.critic.update_critic(data=data)
[36m(main_task pid=48777)[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/Workspace/0_git/TinyZero/verl/workers/critic/dp_critic.py", line 180, in update_critic
[36m(main_task pid=48777)[0m     vpreds = self._forward_micro_batch(data)
[36m(main_task pid=48777)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/Workspace/0_git/TinyZero/verl/workers/critic/dp_critic.py", line 95, in _forward_micro_batch
[36m(main_task pid=48777)[0m     output = self.critic_module(input_ids=input_ids,
[36m(main_task pid=48777)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=48777)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=48777)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[36m(main_task pid=48777)[0m     output = self._fsdp_wrapped_module(*args, **kwargs)
[36m(main_task pid=48777)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=48777)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=48777)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1380, in forward
[36m(main_task pid=48777)[0m     outputs = self.model(
[36m(main_task pid=48777)[0m               ^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=48777)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=48777)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 895, in forward
[36m(main_task pid=48777)[0m     layer_outputs = decoder_layer(
[36m(main_task pid=48777)[0m                     ^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=48777)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=48777)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[36m(main_task pid=48777)[0m     output = self._fsdp_wrapped_module(*args, **kwargs)
[36m(main_task pid=48777)[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=48777)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=48777)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 638, in forward
[36m(main_task pid=48777)[0m     hidden_states = self.mlp(hidden_states)
[36m(main_task pid=48777)[0m                     ^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=48777)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=48777)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 223, in forward
[36m(main_task pid=48777)[0m     return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))
[36m(main_task pid=48777)[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=48777)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=48777)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m   File "/home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 117, in forward
[36m(main_task pid=48777)[0m     return F.linear(input, self.weight, self.bias)
[36m(main_task pid=48777)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(main_task pid=48777)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 45.12 MiB is free. Including non-PyTorch memory, this process has 23.60 GiB memory in use. Of the allocated memory 23.07 GiB is allocated by PyTorch, and 106.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(WorkerDict pid=49176)[0m /home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=49176)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=49176)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=49176)[0m /home/zengyu/anaconda3/envs/tinyzero/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=49176)[0m   warnings.warn(
